{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we do a basic EDA on the image data and features revealed with the help of the Detectron2 Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must register our images and annotation files into the Detectron2 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_coco_instances(\"train_lane_cone_detector\", {}, \"../test/testdata/train/train.json\", \"../test/testdata/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load in the saved metadata and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/07 19:24:14 d2.data.datasets.coco]: \u001b[0mLoaded 2 images in COCO format from ../test/testdata/train/train.json\n"
     ]
    }
   ],
   "source": [
    "train_dataset_metadata = MetadataCatalog.get(\"train_lane_cone_detector\")\n",
    "train_dataset_dicts = DatasetCatalog.get(\"train_lane_cone_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the metadata contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(name='train_lane_cone_detector',\n",
       "          json_file='../test/testdata/train/train.json',\n",
       "          image_root='../test/testdata/train',\n",
       "          evaluator_type='coco',\n",
       "          thing_classes=['lane', 'cone'],\n",
       "          thing_dataset_id_to_contiguous_id={1: 0, 2: 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it contains a lot of information about our dataset. Namely the location of the annotation files and the location of the images. It also was able to autodetect that are images are annotated in the COCO format (Our .json file is based of the COCO JSON captioning). Lastly it knows the possible labels our objects have (lane and cone) and has assigned integers to them for the Detectron2 network to use in calculating metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's next take a look at the dictionary saved from the DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_name': '../test/testdata/train/frame0034.jpg',\n",
       "  'height': 360,\n",
       "  'width': 1280,\n",
       "  'image_id': 1,\n",
       "  'annotations': [{'iscrowd': 0,\n",
       "    'bbox': [1.103448275862069,\n",
       "     102.20689655172414,\n",
       "     395.03448275862064,\n",
       "     243.86206896551727],\n",
       "    'category_id': 0,\n",
       "    'segmentation': [[1.103448275862069,\n",
       "      311.8620689655172,\n",
       "      1.103448275862069,\n",
       "      346.0689655172414,\n",
       "      328.82758620689657,\n",
       "      171.72413793103448,\n",
       "      396.13793103448273,\n",
       "      115.44827586206897,\n",
       "      387.3103448275862,\n",
       "      102.20689655172414,\n",
       "      366.3448275862069,\n",
       "      105.51724137931035,\n",
       "      366.3448275862069,\n",
       "      123.17241379310344]],\n",
       "    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n",
       "   {'iscrowd': 0,\n",
       "    'bbox': [987.5862068965517,\n",
       "     76.82758620689656,\n",
       "     290.20689655172407,\n",
       "     55.172413793103445],\n",
       "    'category_id': 0,\n",
       "    'segmentation': [[1276.6896551724137,\n",
       "      132,\n",
       "      1277.7931034482758,\n",
       "      120.9655172413793,\n",
       "      987.5862068965517,\n",
       "      76.82758620689656,\n",
       "      989.7931034482758,\n",
       "      85.6551724137931]],\n",
       "    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n",
       "   {'iscrowd': 0,\n",
       "    'bbox': [830.8965517241379,\n",
       "     1.793103448275862,\n",
       "     208.55172413793093,\n",
       "     129.10344827586206],\n",
       "    'category_id': 1,\n",
       "    'segmentation': [[830.8965517241379,\n",
       "      115.44827586206897,\n",
       "      965.5172413793103,\n",
       "      130.89655172413794,\n",
       "      1039.4482758620688,\n",
       "      104.41379310344827,\n",
       "      997.5172413793103,\n",
       "      96.6896551724138,\n",
       "      963.3103448275862,\n",
       "      1.793103448275862,\n",
       "      888.2758620689655,\n",
       "      1.793103448275862,\n",
       "      876.1379310344828,\n",
       "      92.27586206896551]],\n",
       "    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n",
       " {'file_name': '../test/testdata/train/frame0035.jpg',\n",
       "  'height': 360,\n",
       "  'width': 1280,\n",
       "  'image_id': 2,\n",
       "  'annotations': [{'iscrowd': 0,\n",
       "    'bbox': [162.20689655172413,\n",
       "     118.75862068965517,\n",
       "     253.79310344827587,\n",
       "     238.34482758620686],\n",
       "    'category_id': 0,\n",
       "    'segmentation': [[162.20689655172413,\n",
       "      357.10344827586204,\n",
       "      247.17241379310343,\n",
       "      356,\n",
       "      408.2758620689655,\n",
       "      169.51724137931035,\n",
       "      416,\n",
       "      118.75862068965517,\n",
       "      388.41379310344826,\n",
       "      123.17241379310344,\n",
       "      374.0689655172414,\n",
       "      157.3793103448276]],\n",
       "    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n",
       "   {'iscrowd': 0,\n",
       "    'bbox': [756.9655172413793,\n",
       "     56.96551724137931,\n",
       "     518.6206896551723,\n",
       "     58.48275862068966],\n",
       "    'category_id': 0,\n",
       "    'segmentation': [[1274.4827586206895,\n",
       "      115.44827586206897,\n",
       "      1275.5862068965516,\n",
       "      105.51724137931035,\n",
       "      758.0689655172414,\n",
       "      56.96551724137931,\n",
       "      756.9655172413793,\n",
       "      65.79310344827586]],\n",
       "    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we notice that it is saved into a list where each element has the same format. Let's take a look at an element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '../test/testdata/train/frame0034.jpg',\n",
       " 'height': 360,\n",
       " 'width': 1280,\n",
       " 'image_id': 1,\n",
       " 'annotations': [{'iscrowd': 0,\n",
       "   'bbox': [1.103448275862069,\n",
       "    102.20689655172414,\n",
       "    395.03448275862064,\n",
       "    243.86206896551727],\n",
       "   'category_id': 0,\n",
       "   'segmentation': [[1.103448275862069,\n",
       "     311.8620689655172,\n",
       "     1.103448275862069,\n",
       "     346.0689655172414,\n",
       "     328.82758620689657,\n",
       "     171.72413793103448,\n",
       "     396.13793103448273,\n",
       "     115.44827586206897,\n",
       "     387.3103448275862,\n",
       "     102.20689655172414,\n",
       "     366.3448275862069,\n",
       "     105.51724137931035,\n",
       "     366.3448275862069,\n",
       "     123.17241379310344]],\n",
       "   'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n",
       "  {'iscrowd': 0,\n",
       "   'bbox': [987.5862068965517,\n",
       "    76.82758620689656,\n",
       "    290.20689655172407,\n",
       "    55.172413793103445],\n",
       "   'category_id': 0,\n",
       "   'segmentation': [[1276.6896551724137,\n",
       "     132,\n",
       "     1277.7931034482758,\n",
       "     120.9655172413793,\n",
       "     987.5862068965517,\n",
       "     76.82758620689656,\n",
       "     989.7931034482758,\n",
       "     85.6551724137931]],\n",
       "   'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n",
       "  {'iscrowd': 0,\n",
       "   'bbox': [830.8965517241379,\n",
       "    1.793103448275862,\n",
       "    208.55172413793093,\n",
       "    129.10344827586206],\n",
       "   'category_id': 1,\n",
       "   'segmentation': [[830.8965517241379,\n",
       "     115.44827586206897,\n",
       "     965.5172413793103,\n",
       "     130.89655172413794,\n",
       "     1039.4482758620688,\n",
       "     104.41379310344827,\n",
       "     997.5172413793103,\n",
       "     96.6896551724138,\n",
       "     963.3103448275862,\n",
       "     1.793103448275862,\n",
       "     888.2758620689655,\n",
       "     1.793103448275862,\n",
       "     876.1379310344828,\n",
       "     92.27586206896551]],\n",
       "   'bbox_mode': <BoxMode.XYWH_ABS: 1>}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element seems to be talking about a specific image file in dictionary format. The dictionary contains information about the image name, it's height and width and also has an image ID attached to it. It's also contains a lot of details about the annotations, specifically the boundary boxes, segmentations and the category ID each of them belong to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot one of the images now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![raw_image](../data/report/raw_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the image does indeed have a cone and lane object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1280, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(train_dataset_dicts[0][\"file_name\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the image indeed is a 360x1280 image. The 3 reflects 3 channels which is true since this is a BGR image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the boundary box and segmentation results on top of this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![labelled_image](../data/report/labelled_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the iamge has appropriate boundary boxes and segmentations surrounding it as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Detectron2 Network will be trained on such images and then finally use an inference on an unknown set of images to make a video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
